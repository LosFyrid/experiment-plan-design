"""
ACE Reflector - Analyzes generated plans and extracts insights.

Implements ACE paper ยง3 Reflector role with:
- Iterative refinement (max 5 rounds) to improve insight quality
- Error pattern identification
- Root cause analysis
- Bullet tagging (helpful/harmful/neutral)
- Actionable insight extraction for Curator
"""

from typing import List, Dict, Optional
import json
import time
from datetime import datetime

from ace_framework.playbook.schemas import (
    ExperimentPlan,
    TrajectoryStep,
    Feedback,
    ReflectionResult,
    Insight,
    BulletTag
)
from ace_framework.playbook.playbook_manager import PlaybookManager
from utils.llm_provider import BaseLLMProvider, extract_json_from_text
from utils.config_loader import ReflectorConfig
from utils.structured_logger import StructuredLogger, create_reflector_logger
from utils.performance_monitor import PerformanceMonitor
from utils.llm_call_tracker import LLMCallTracker
from .prompts import (
    INITIAL_REFLECTION_SYSTEM_PROMPT,
    REFINEMENT_SYSTEM_PROMPT,
    build_initial_reflection_prompt,
    build_refinement_prompt
)


class PlanReflector:
    """
    ACE Reflector - Analyzes plans and extracts actionable insights.

    Implements the Reflector role from ACE paper ยง3 with iterative
    refinement (Figure 4) to progressively improve insight quality.
    """

    def __init__(
        self,
        llm_provider: BaseLLMProvider,
        config: ReflectorConfig,
        playbook_manager: Optional[PlaybookManager] = None,
        logger: Optional[StructuredLogger] = None,
        perf_monitor: Optional[PerformanceMonitor] = None,
        llm_tracker: Optional[LLMCallTracker] = None
    ):
        """
        Args:
            llm_provider: LLM provider for reflection
            config: Reflector configuration
            playbook_manager: Optional manager for bullet content lookup
            logger: Optional structured logger (created if None)
            perf_monitor: Optional performance monitor
            llm_tracker: Optional LLM call tracker
        """
        self.llm = llm_provider
        self.config = config
        self.playbook_manager = playbook_manager

        # Observability tools
        self.logger = logger or create_reflector_logger()
        self.perf_monitor = perf_monitor
        self.llm_tracker = llm_tracker

    # ========================================================================
    # Main Reflection Method
    # ========================================================================

    def reflect(
        self,
        generated_plan: ExperimentPlan,
        feedback: Feedback,
        trajectory: List[TrajectoryStep],
        playbook_bullets_used: List[str],
        ground_truth: Optional[ExperimentPlan] = None
    ) -> ReflectionResult:
        """
        Analyze generated plan and extract insights.

        Implements ACE Reflector with iterative refinement (ยง3, Figure 4).

        Args:
            generated_plan: Experiment plan generated by Generator
            feedback: Evaluation feedback (from LLM judge or human)
            trajectory: Reasoning trajectory from Generator
            playbook_bullets_used: List of bullet IDs referenced
            ground_truth: Optional ground truth plan for comparison

        Returns:
            ReflectionResult with insights and bullet tags

        Raises:
            RuntimeError: If reflection fails
        """
        reflection_start_time = time.time()

        # Log reflection start
        self.logger.log_reflection_started(
            plan_title=generated_plan.title,
            feedback_score=feedback.overall_score,
            max_refinement_rounds=self.config.max_refinement_rounds
        )

        # Get bullet contents for context
        bullet_contents = self._get_bullet_contents(playbook_bullets_used)

        # Step 1: Initial reflection
        if self.perf_monitor:
            with self.perf_monitor.measure("initial_reflection", "reflector"):
                initial_start = time.time()
                initial_output = self._perform_initial_reflection(
                    plan=generated_plan,
                    feedback=feedback,
                    trajectory=trajectory,
                    bullets_used=playbook_bullets_used,
                    bullet_contents=bullet_contents
                )
                initial_duration = time.time() - initial_start
        else:
            initial_start = time.time()
            initial_output = self._perform_initial_reflection(
                plan=generated_plan,
                feedback=feedback,
                trajectory=trajectory,
                bullets_used=playbook_bullets_used,
                bullet_contents=bullet_contents
            )
            initial_duration = time.time() - initial_start

        # Log initial reflection completion
        initial_insights = initial_output.get("insights", [])
        priority_dist = {"high": 0, "medium": 0, "low": 0}
        for insight in initial_insights:
            priority = insight.get("priority", "medium")
            priority_dist[priority] = priority_dist.get(priority, 0) + 1

        self.logger.log_initial_reflection_done(
            duration=initial_duration,
            insights_count=len(initial_insights),
            priority_distribution=priority_dist
        )

        # Step 2: Iterative refinement (if enabled)
        refinement_rounds_completed = 1
        if self.config.enable_iterative and self.config.max_refinement_rounds > 1:
            refined_output = self._perform_iterative_refinement(
                initial_output=initial_output,
                max_rounds=self.config.max_refinement_rounds
            )
            refinement_rounds_completed = self.config.max_refinement_rounds
        else:
            refined_output = initial_output

        # Step 3: Parse final output
        insights = self._parse_insights(refined_output.get("insights", []))
        bullet_tags = self._parse_bullet_tags(refined_output.get("bullet_tags", {}))

        # Log bullet tagging
        tag_counts = {"helpful": 0, "harmful": 0, "neutral": 0}
        for tag in bullet_tags.values():
            if tag == BulletTag.HELPFUL:
                tag_counts["helpful"] += 1
            elif tag == BulletTag.HARMFUL:
                tag_counts["harmful"] += 1
            elif tag == BulletTag.NEUTRAL:
                tag_counts["neutral"] += 1

        self.logger.log_bullet_tagging_done(
            tags={bid: tag.value for bid, tag in bullet_tags.items()},
            helpful=tag_counts["helpful"],
            harmful=tag_counts["harmful"],
            neutral=tag_counts["neutral"]
        )

        # Step 4: Extract error analysis
        error_analysis = self._format_error_analysis(refined_output)

        # Step 5: Create result
        total_duration = time.time() - reflection_start_time

        result = ReflectionResult(
            insights=insights,
            bullet_tags=bullet_tags,
            error_analysis=error_analysis,
            refinement_rounds=refinement_rounds_completed,
            reflection_metadata={
                "model": self.llm.model_name if hasattr(self.llm, 'model_name') else "unknown",
                "feedback_score": feedback.overall_score,
                "feedback_source": feedback.feedback_source,
                "bullets_analyzed": len(playbook_bullets_used),
                "duration": total_duration
            },
            timestamp=datetime.now()
        )

        # Log insights extraction completion
        self.logger.log_insights_extracted(
            total_duration=total_duration,
            final_insights_count=len(insights),
            refinement_rounds_completed=refinement_rounds_completed
        )

        return result

    # ========================================================================
    # Initial Reflection
    # ========================================================================

    def _perform_initial_reflection(
        self,
        plan: ExperimentPlan,
        feedback: Feedback,
        trajectory: List[TrajectoryStep],
        bullets_used: List[str],
        bullet_contents: Dict[str, str]
    ) -> Dict:
        """
        Perform initial reflection (Round 1).

        Args:
            plan: Generated experiment plan
            feedback: Evaluation feedback
            trajectory: Reasoning trajectory
            bullets_used: Bullet IDs used
            bullet_contents: Bullet ID -> content mapping

        Returns:
            Parsed reflection output dict

        Raises:
            RuntimeError: If reflection fails
        """
        # Build prompt
        prompt = build_initial_reflection_prompt(
            plan=plan,
            feedback=feedback,
            trajectory=trajectory,
            bullets_used=bullets_used,
            bullet_contents=bullet_contents
        )

        # Track LLM call
        llm_call_id = None
        if self.llm_tracker:
            llm_call_id = self.llm_tracker.start_call(
                model_provider=getattr(self.llm, 'provider', 'unknown'),
                model_name=getattr(self.llm, 'model_name', 'unknown'),
                config={
                    "temperature": getattr(self.llm, 'temperature', 0.7),
                    "max_tokens": getattr(self.llm, 'max_tokens', 4000)
                },
                system_prompt=INITIAL_REFLECTION_SYSTEM_PROMPT,
                user_prompt=prompt,
                stage="reflection",
                detail="initial"
            )

        # Generate reflection
        try:
            response = self.llm.generate(
                prompt=prompt,
                system_prompt=INITIAL_REFLECTION_SYSTEM_PROMPT
            )

            # Complete LLM call tracking
            if self.llm_tracker and llm_call_id:
                self.llm_tracker.end_call(
                    response=response.content,
                    input_tokens=getattr(response, 'prompt_tokens', None),
                    output_tokens=getattr(response, 'completion_tokens', None),
                    status="success"
                )

            # Record tokens
            if self.perf_monitor and hasattr(response, 'prompt_tokens'):
                self.perf_monitor.record_tokens(
                    component="reflector",
                    input_tokens=response.prompt_tokens,
                    output_tokens=response.completion_tokens,
                    call_id=llm_call_id
                )

        except Exception as e:
            if self.llm_tracker and llm_call_id:
                self.llm_tracker.end_call(response="", status="error", error=str(e))
            raise RuntimeError(f"Initial reflection failed: {e}")

        # Parse output
        return self._parse_reflection_output(response.content)

    # ========================================================================
    # Iterative Refinement
    # ========================================================================

    def _perform_iterative_refinement(
        self,
        initial_output: Dict,
        max_rounds: int
    ) -> Dict:
        """
        Perform iterative refinement to improve insight quality.

        Implements ACE paper ยง3, Figure 4.

        Args:
            initial_output: Output from initial reflection
            max_rounds: Maximum refinement rounds (typically 5)

        Returns:
            Refined output dict
        """
        current_output = initial_output

        # Refinement rounds (2 to max_rounds)
        for round_num in range(2, max_rounds + 1):
            # Log round start
            self.logger.log_refinement_round_started(round_num)

            round_start = time.time()

            # Build refinement prompt
            prompt = build_refinement_prompt(
                previous_insights=current_output.get("insights", []),
                previous_analysis=current_output.get("error_identification", ""),
                round_number=round_num,
                max_rounds=max_rounds
            )

            # Track LLM call
            llm_call_id = None
            if self.llm_tracker:
                llm_call_id = self.llm_tracker.start_call(
                    model_provider=getattr(self.llm, 'provider', 'unknown'),
                    model_name=getattr(self.llm, 'model_name', 'unknown'),
                    config={
                        "temperature": getattr(self.llm, 'temperature', 0.7),
                        "max_tokens": getattr(self.llm, 'max_tokens', 4000)
                    },
                    system_prompt=REFINEMENT_SYSTEM_PROMPT,
                    user_prompt=prompt,
                    stage="refinement",
                    detail=f"round{round_num}"
                )

            # Generate refinement
            try:
                if self.perf_monitor:
                    with self.perf_monitor.measure(f"refinement_round_{round_num}", "reflector"):
                        response = self.llm.generate(
                            prompt=prompt,
                            system_prompt=REFINEMENT_SYSTEM_PROMPT
                        )
                else:
                    response = self.llm.generate(
                        prompt=prompt,
                        system_prompt=REFINEMENT_SYSTEM_PROMPT
                    )

                # Complete LLM call tracking
                if self.llm_tracker and llm_call_id:
                    self.llm_tracker.end_call(
                        response=response.content,
                        input_tokens=getattr(response, 'prompt_tokens', None),
                        output_tokens=getattr(response, 'completion_tokens', None),
                        status="success"
                    )

                # Record tokens
                if self.perf_monitor and hasattr(response, 'prompt_tokens'):
                    self.perf_monitor.record_tokens(
                        component="reflector",
                        input_tokens=response.prompt_tokens,
                        output_tokens=response.completion_tokens,
                        call_id=llm_call_id
                    )

            except Exception as e:
                if self.llm_tracker and llm_call_id:
                    self.llm_tracker.end_call(response="", status="error", error=str(e))
                print(f"Warning: Refinement round {round_num} failed: {e}")
                # Keep previous output
                break

            # Parse refined output
            try:
                refined_output = self._parse_reflection_output(response.content)

                # Check if quality improved (simple heuristic: more/same insights)
                quality_improved = len(refined_output.get("insights", [])) >= len(current_output.get("insights", []))

                current_output = refined_output

                # Log round completion
                round_duration = time.time() - round_start
                self.logger.log_refinement_round_completed(
                    round_num=round_num,
                    duration=round_duration,
                    insights_count=len(refined_output.get("insights", [])),
                    quality_improved=quality_improved
                )

            except Exception as e:
                print(f"Warning: Failed to parse refinement round {round_num}: {e}")
                # Keep previous output
                break

        return current_output

    # ========================================================================
    # Parsing Methods
    # ========================================================================

    def _parse_reflection_output(self, content: str) -> Dict:
        """
        Parse reflection LLM output.

        Args:
            content: LLM response content

        Returns:
            Parsed dict with reflection results

        Raises:
            ValueError: If parsing fails
        """
        # Remove markdown code blocks
        content_stripped = content.strip()

        if content_stripped.startswith("```json"):
            content_stripped = content_stripped[7:]
        elif content_stripped.startswith("```"):
            content_stripped = content_stripped[3:]

        if content_stripped.endswith("```"):
            content_stripped = content_stripped[:-3]

        content_stripped = content_stripped.strip()

        # Try direct parse
        try:
            return json.loads(content_stripped)
        except json.JSONDecodeError:
            pass

        # Try extraction
        extracted = extract_json_from_text(content)
        if extracted:
            return extracted

        raise ValueError(f"Failed to parse reflection output.\nContent: {content[:500]}...")

    def _parse_insights(self, insights_data: List[Dict]) -> List[Insight]:
        """
        Parse insights from reflection output.

        Args:
            insights_data: List of insight dicts

        Returns:
            List of Insight objects
        """
        insights = []

        for insight_dict in insights_data:
            try:
                insight = Insight(
                    type=insight_dict.get("type", "general"),
                    description=insight_dict.get("description", ""),
                    suggested_bullet=insight_dict.get("suggested_bullet"),
                    target_section=insight_dict.get("target_section"),
                    priority=insight_dict.get("priority", "medium")
                )
                insights.append(insight)
            except Exception as e:
                print(f"Warning: Failed to parse insight: {e}")
                continue

        return insights

    def _parse_bullet_tags(self, tags_data: Dict[str, str]) -> Dict[str, BulletTag]:
        """
        Parse bullet tags from reflection output.

        Args:
            tags_data: Dict of bullet_id -> tag_string

        Returns:
            Dict of bullet_id -> BulletTag enum
        """
        bullet_tags = {}

        for bullet_id, tag_str in tags_data.items():
            try:
                # Normalize tag string
                tag_str = tag_str.lower().strip()

                if tag_str == "helpful":
                    bullet_tags[bullet_id] = BulletTag.HELPFUL
                elif tag_str == "harmful":
                    bullet_tags[bullet_id] = BulletTag.HARMFUL
                elif tag_str == "neutral":
                    bullet_tags[bullet_id] = BulletTag.NEUTRAL
                else:
                    print(f"Warning: Unknown bullet tag '{tag_str}' for {bullet_id}")
                    bullet_tags[bullet_id] = BulletTag.NEUTRAL

            except Exception as e:
                print(f"Warning: Failed to parse tag for {bullet_id}: {e}")
                continue

        return bullet_tags

    def _format_error_analysis(self, reflection_output: Dict) -> str:
        """
        Format error analysis from reflection output.

        Args:
            reflection_output: Parsed reflection dict

        Returns:
            Formatted error analysis string
        """
        sections = []

        if "error_identification" in reflection_output:
            sections.append("## Error Identification")
            sections.append(reflection_output["error_identification"])

        if "root_cause_analysis" in reflection_output:
            sections.append("\n## Root Cause Analysis")
            sections.append(reflection_output["root_cause_analysis"])

        if "correct_approach" in reflection_output:
            sections.append("\n## Correct Approach")
            sections.append(reflection_output["correct_approach"])

        return "\n\n".join(sections) if sections else "No error analysis provided."

    # ========================================================================
    # Utilities
    # ========================================================================

    def _get_bullet_contents(self, bullet_ids: List[str]) -> Dict[str, str]:
        """
        Get bullet contents for given IDs.

        Args:
            bullet_ids: List of bullet IDs

        Returns:
            Dict mapping bullet_id -> content
        """
        if not self.playbook_manager:
            return {bid: "Content not available" for bid in bullet_ids}

        bullet_contents = {}

        for bullet_id in bullet_ids:
            bullet = self.playbook_manager.playbook.get_bullet_by_id(bullet_id)
            if bullet:
                bullet_contents[bullet_id] = bullet.content
            else:
                bullet_contents[bullet_id] = "Bullet not found"

        return bullet_contents


# ============================================================================
# Factory Function
# ============================================================================

def create_reflector(
    llm_provider: BaseLLMProvider,
    config: Optional[ReflectorConfig] = None,
    playbook_manager: Optional[PlaybookManager] = None
) -> PlanReflector:
    """
    Factory function to create PlanReflector.

    Args:
        llm_provider: LLM provider instance
        config: Reflector configuration (uses default if None)
        playbook_manager: Optional playbook manager for bullet lookup

    Returns:
        Configured PlanReflector instance
    """
    from ...utils.config_loader import get_ace_config

    if config is None:
        config = get_ace_config().reflector

    return PlanReflector(
        llm_provider=llm_provider,
        config=config,
        playbook_manager=playbook_manager
    )
