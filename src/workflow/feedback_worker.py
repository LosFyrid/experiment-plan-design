"""åé¦ˆå·¥ä½œè¿›ç¨‹ - ç‹¬ç«‹è¿è¡Œ

ç”¨æ³•:
    python -m workflow.feedback_worker <task_id>
    python -m workflow.feedback_worker <task_id> --mode <evaluation_mode>

æµç¨‹:
    COMPLETED â†’ EVALUATING â†’ REFLECTING â†’ CURATING â†’ FEEDBACK_COMPLETED

ç‰¹æ€§:
- ä½œä¸ºç‹¬ç«‹è¿›ç¨‹è¿è¡Œï¼ˆå¯è„±ç¦»ä¸»CLIï¼‰
- æ‰€æœ‰printè¾“å‡ºè¢«çˆ¶è¿›ç¨‹ç®¡é“æ•è·
- çŠ¶æ€æ›´æ–°å†™å…¥æ–‡ä»¶ç³»ç»Ÿï¼ˆtask.jsonï¼‰
- ä»generation_result.jsonåŠ è½½å®Œæ•´æ•°æ®ï¼ˆtrajectory + relevant_bulletsï¼‰
"""

import sys
import json
import argparse
import os
from pathlib import Path
from typing import Optional

# ç¡®ä¿ src åœ¨ Python è·¯å¾„ä¸­
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent  # src/workflow -> src
project_root = src_dir.parent  # src -> é¡¹ç›®æ ¹

if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")


def main():
    """åé¦ˆå·¥ä½œè¿›ç¨‹ä¸»å…¥å£"""
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description="åé¦ˆè®­ç»ƒå·¥ä½œè¿›ç¨‹")
    parser.add_argument("task_id", help="ä»»åŠ¡ID")
    parser.add_argument("--mode", default=None,
                       choices=["auto", "llm_judge", "human"],
                       help="è¯„ä¼°æ¨¡å¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰")
    args = parser.parse_args()

    task_id = args.task_id
    evaluation_mode = args.mode

    print("=" * 70)
    print("ğŸ”§ Feedback Worker Started")
    print("=" * 70)
    print(f"  Task ID: {task_id}")
    print(f"  Evaluation Mode: {evaluation_mode or '(from config)'}")
    print(f"  PID: {os.getpid()}")
    print()

    try:
        # æ‰§è¡Œåé¦ˆå·¥ä½œæµ
        run_feedback_workflow(task_id, evaluation_mode)

    except Exception as e:
        import traceback
        print()
        print("=" * 70)
        print(f"âŒ Feedback Worker å¼‚å¸¸: {e}")
        print("=" * 70)
        traceback.print_exc()

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            from workflow.task_manager import get_task_manager, TaskStatus
            tm = get_task_manager()
            task = tm.get_task(task_id)
            if task:
                task.status = TaskStatus.FAILED
                task.error = f"åé¦ˆæµç¨‹å¤±è´¥: {str(e)}"
                tm._save_task(task)
        except:
            pass

        sys.exit(1)


def run_feedback_workflow(task_id: str, evaluation_mode: Optional[str] = None):
    """è¿è¡Œåé¦ˆå·¥ä½œæµï¼ˆçŠ¶æ€æœºæ¨¡å¼ï¼‰

    Args:
        task_id: ä»»åŠ¡ID
        evaluation_mode: è¯„ä¼°æ¨¡å¼ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä»é…ç½®è¯»å–

    æµç¨‹:
        1. éªŒè¯ä»»åŠ¡çŠ¶æ€ï¼ˆå¿…é¡»æ˜¯COMPLETEDï¼‰
        2. åŠ è½½generation_result.jsonï¼ˆplan + trajectory + relevant_bulletsï¼‰
        3. è¯„ä¼°æ–¹æ¡ˆï¼ˆEVALUATINGï¼‰
        4. åæ€åˆ†æï¼ˆREFLECTINGï¼‰
        5. æ›´æ–°playbookï¼ˆCURATINGï¼‰
        6. å®Œæˆï¼ˆFEEDBACK_COMPLETEDï¼‰
    """
    from workflow.task_manager import get_task_manager, TaskStatus
    from utils.config_loader import get_ace_config

    # 1. åŠ è½½ä»»åŠ¡
    task_manager = get_task_manager()
    task = task_manager.get_task(task_id)

    if not task:
        print(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
        sys.exit(1)

    # éªŒè¯çŠ¶æ€
    if task.status != TaskStatus.COMPLETED:
        print(f"âŒ ä»»åŠ¡çŠ¶æ€ä¸º {task.status.value}ï¼Œåªèƒ½å¯¹å·²å®Œæˆçš„ä»»åŠ¡è¿›è¡Œåé¦ˆ")
        sys.exit(1)

    print(f"[Worker] ä»»åŠ¡å·²å®Œæˆï¼Œå¼€å§‹åé¦ˆè®­ç»ƒæµç¨‹")

    # 2. ä»é…ç½®è¯»å–é»˜è®¤è¯„ä¼°æ¨¡å¼ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if evaluation_mode is None:
        ace_config = get_ace_config()
        evaluation_mode = ace_config.training.feedback_source
        print(f"[Worker] ä½¿ç”¨é…ç½®çš„è¯„ä¼°æ¨¡å¼: {evaluation_mode}")
    else:
        print(f"[Worker] ä½¿ç”¨æŒ‡å®šçš„è¯„ä¼°æ¨¡å¼: {evaluation_mode}")

    # 3. ä» generation_result.json åŠ è½½å®Œæ•´æ•°æ®
    print()
    print("=" * 70)
    print("åŠ è½½ç”Ÿæˆç»“æœ")
    print("=" * 70)

    generation_result_data = task.load_generation_result()

    if not generation_result_data:
        # æ—§ä»»åŠ¡ï¼ˆæ²¡æœ‰ generation_result.jsonï¼‰
        print("âš ï¸  æ—§ä»»åŠ¡ç¼ºå°‘ generation_result.jsonï¼Œè¿›è¡Œé™çº§å¤„ç†")

        # é™çº§ï¼šåªåŠ è½½ plan
        with open(task.plan_file, 'r', encoding='utf-8') as f:
            plan_data = json.load(f)

        from ace_framework.playbook.schemas import ExperimentPlan
        plan = ExperimentPlan(**plan_data)

        trajectory = []
        relevant_bullets = []

        print(f"  âš ï¸  Trajectory: ä¸å¯ç”¨ï¼ˆæ—§ä»»åŠ¡ï¼‰")
        print(f"  âš ï¸  Relevant bullets: ä¸å¯ç”¨ï¼ˆæ—§ä»»åŠ¡ï¼‰")
        print(f"  â„¹ï¸  åé¦ˆåŠŸèƒ½å°†å—é™")
    else:
        # æ–°ä»»åŠ¡ï¼ˆæœ‰å®Œæ•´æ•°æ®ï¼‰
        from ace_framework.playbook.schemas import ExperimentPlan, TrajectoryStep

        plan = ExperimentPlan(**generation_result_data["plan"])

        # æå– trajectoryï¼ˆç»™ Reflector åˆ†ææ¨ç†è¿‡ç¨‹ï¼‰
        trajectory = [
            TrajectoryStep(**step)
            for step in generation_result_data.get("trajectory", [])
        ]

        # æå– relevant_bulletsï¼ˆç»™ Reflector æ ‡è®°æœ‰ç”¨/æœ‰å®³çš„ bulletsï¼‰
        relevant_bullets = generation_result_data.get("relevant_bullets", [])

        print(f"  âœ… Plan: {plan.title}")
        print(f"  âœ… Trajectory æ­¥éª¤: {len(trajectory)}")
        print(f"  âœ… Relevant bullets: {len(relevant_bullets)}")

        if trajectory:
            print(f"  â„¹ï¸  Trajectory é¢„è§ˆ: {trajectory[0].thought[:60]}...")
        if relevant_bullets:
            print(f"  â„¹ï¸  Bullets é¢„è§ˆ: {', '.join(relevant_bullets[:5])}")

    # 4. åˆå§‹åŒ–ç»„ä»¶
    print()
    print("=" * 70)
    print("åˆå§‹åŒ–ACEç»„ä»¶")
    print("=" * 70)

    # LLM Provider
    print("[1/4] æ­£åœ¨åˆå§‹åŒ–LLM Provider...")
    from utils.llm_provider import QwenProvider

    llm_provider = QwenProvider(
        model_name="qwen-max",
        temperature=0.7,
        max_tokens=4096
    )
    print("  âœ… Qwen Provider å·²åˆå§‹åŒ–")

    # Playbook Manager
    print("[2/4] æ­£åœ¨åŠ è½½Playbook...")
    from ace_framework.playbook.playbook_manager import PlaybookManager

    playbook_path = "data/playbooks/chemistry_playbook.json"
    playbook_manager = PlaybookManager(playbook_path=playbook_path)
    playbook_manager.load()
    print(f"  âœ… Playbook å·²åŠ è½½: {playbook_manager.playbook.size} bullets")

    # Reflector
    print("[3/4] æ­£åœ¨åˆå§‹åŒ–Reflector...")
    from ace_framework.reflector.reflector import PlanReflector
    from utils.config_loader import ReflectorConfig

    reflector = PlanReflector(
        llm_provider=llm_provider,
        config=ReflectorConfig(),
        playbook_manager=playbook_manager
    )
    print("  âœ… Reflector å·²åˆå§‹åŒ–")

    # Curator
    print("[4/4] æ­£åœ¨åˆå§‹åŒ–Curator...")
    from ace_framework.curator.curator import PlaybookCurator
    from utils.config_loader import CuratorConfig

    curator = PlaybookCurator(
        playbook_manager=playbook_manager,
        llm_provider=llm_provider,
        config=CuratorConfig()
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ allow_new_sections è®¾ç½®ï¼ˆconfigs/playbook_sections.yamlï¼‰
        # ä¸åœ¨è¿è¡Œæ—¶è¦†ç›–ï¼Œç¡®ä¿è¡Œä¸ºå¯é¢„æµ‹ä¸”é…ç½®ç»Ÿä¸€
    )
    print("  âœ… Curator å·²åˆå§‹åŒ–")

    # ========================================================================
    # Step 1: è¯„ä¼°
    # ========================================================================
    task.feedback_status = "evaluating"
    task_manager._save_task(task)

    print()
    print("=" * 70)
    print("STEP 1: è¯„ä¼°æ–¹æ¡ˆè´¨é‡")
    print("=" * 70)

    from evaluation.evaluator import evaluate_plan

    try:
        feedback = evaluate_plan(
            plan=plan,
            source=evaluation_mode,
            llm_provider=llm_provider if evaluation_mode == "llm_judge" else None
        )

        task.save_feedback(feedback)
        print(f"  âœ… è¯„ä¼°å®Œæˆ: {feedback.overall_score:.2f}")

        # æ˜¾ç¤ºè¯„åˆ†è¯¦æƒ…
        print(f"\n  è¯„åˆ†è¯¦æƒ…:")
        for score in feedback.scores:
            print(f"    - {score.criterion}: {score.score:.2f}")
        print(f"  è¯„è®º: {feedback.comments}")
        print(f"  âœ… åé¦ˆå·²ä¿å­˜: {task.feedback_file}")

    except Exception as e:
        # Feedbackæµç¨‹å¤±è´¥ï¼Œä¸å½±å“ä¸»ä»»åŠ¡
        task.feedback_status = "failed"
        task.feedback_error = f"è¯„ä¼°å¤±è´¥: {str(e)}"
        task.failed_stage = "evaluating"  # è®°å½•å¤±è´¥é˜¶æ®µ
        task.status = TaskStatus.FAILED  # æ ‡è®°æ•´ä½“å¤±è´¥ï¼ˆç”¨äºretryï¼‰
        task.error = task.feedback_error
        task_manager._save_task(task)
        print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # ========================================================================
    # Step 2: åæ€
    # ========================================================================
    task.feedback_status = "reflecting"
    task_manager._save_task(task)

    print()
    print("=" * 70)
    print("STEP 2: åæ€åˆ†æ")
    print("=" * 70)

    try:
        reflection_result = reflector.reflect(
            generated_plan=plan,
            feedback=feedback,
            trajectory=trajectory,
            playbook_bullets_used=relevant_bullets,
            verbose=True  # å¯ç”¨è¯¦ç»†è¿›åº¦è¾“å‡º
        )

        task.save_reflection(reflection_result)
        print(f"  âœ… æå–äº† {len(reflection_result.insights)} ä¸ª insights")
        print(f"  âœ… æ ‡è®°äº† {len(reflection_result.bullet_tags)} ä¸ª bullets")

        # æ˜¾ç¤ºinsightsç¤ºä¾‹
        if reflection_result.insights:
            print(f"\n  ç¤ºä¾‹insight:")
            for i, insight in enumerate(reflection_result.insights[:3], 1):
                print(f"    {i}. [{insight.type}] {insight.description[:60]}...")
                print(f"       ä¼˜å…ˆçº§: {insight.priority}")

        # æ˜¾ç¤ºbulletæ ‡è®°ç»Ÿè®¡
        if reflection_result.bullet_tags:
            helpful_count = sum(1 for tag in reflection_result.bullet_tags.values() if tag == "helpful")
            harmful_count = sum(1 for tag in reflection_result.bullet_tags.values() if tag == "harmful")
            neutral_count = sum(1 for tag in reflection_result.bullet_tags.values() if tag == "neutral")
            print(f"\n  Bullet æ ‡è®°ç»Ÿè®¡:")
            print(f"    - Helpful: {helpful_count}")
            print(f"    - Harmful: {harmful_count}")
            print(f"    - Neutral: {neutral_count}")

        print(f"  âœ… åæ€ç»“æœå·²ä¿å­˜: {task.reflection_file}")

    except Exception as e:
        # Feedbackæµç¨‹å¤±è´¥
        task.feedback_status = "failed"
        task.feedback_error = f"åæ€å¤±è´¥: {str(e)}"
        task.failed_stage = "reflecting"  # è®°å½•å¤±è´¥é˜¶æ®µ
        task.status = TaskStatus.FAILED
        task.error = task.feedback_error
        task_manager._save_task(task)
        print(f"  âŒ åæ€å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # ========================================================================
    # Step 3: æ›´æ–° Playbook
    # ========================================================================
    task.feedback_status = "curating"
    task_manager._save_task(task)

    print()
    print("=" * 70)
    print("STEP 3: æ›´æ–° Playbook")
    print("=" * 70)

    size_before = playbook_manager.playbook.size

    try:
        curation_result = curator.update(
            reflection_result=reflection_result,
            verbose=True  # å¯ç”¨è¯¦ç»†è¿›åº¦è¾“å‡º
        )

        size_after = playbook_manager.playbook.size

        task.save_curation(curation_result)
        print(f"  âœ… Playbookå·²æ›´æ–°: {size_before} â†’ {size_after} bullets")
        print(f"  âœ… æ“ä½œ: +{curation_result.bullets_added} "
              f"-{curation_result.bullets_removed} ~{curation_result.bullets_updated}")

        # æ˜¾ç¤ºæ–°å¢bulletsç¤ºä¾‹
        if curation_result.bullets_added > 0:
            print(f"\n  æ–°å¢bulletsé¢„è§ˆ:")
            # è·å–æœ€æ–°çš„bulletsï¼ˆbulletsæ˜¯listï¼Œä¸æ˜¯dictï¼‰
            new_bullets = playbook_manager.playbook.bullets[-min(3, curation_result.bullets_added):]
            for bullet in new_bullets:
                print(f"    - [{bullet.section}] {bullet.content[:60]}...")

        print(f"  âœ… æ›´æ–°è®°å½•å·²ä¿å­˜: {task.curation_file}")

    except Exception as e:
        # Feedbackæµç¨‹å¤±è´¥
        task.feedback_status = "failed"
        task.feedback_error = f"Playbookæ›´æ–°å¤±è´¥: {str(e)}"
        task.failed_stage = "curating"  # è®°å½•å¤±è´¥é˜¶æ®µ
        task.status = TaskStatus.FAILED
        task.error = task.feedback_error
        task_manager._save_task(task)
        print(f"  âŒ Playbookæ›´æ–°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # ========================================================================
    # å®Œæˆ
    # ========================================================================
    task.feedback_status = "completed"
    task_manager._save_task(task)

    print()
    print("=" * 70)
    print("âœ… åé¦ˆæµç¨‹å®Œæˆï¼")
    print("=" * 70)
    print(f"\nåé¦ˆæ•°æ®:")
    print(f"  - è¯„ä¼°åé¦ˆ: {task.feedback_file}")
    print(f"  - åæ€ç»“æœ: {task.reflection_file}")
    print(f"  - æ›´æ–°è®°å½•: {task.curation_file}")
    print(f"\nPlaybook å˜åŒ–:")
    print(f"  - å¤§å°: {size_before} â†’ {size_after} bullets")
    print(f"  - æ–°å¢: {curation_result.bullets_added}")
    print(f"  - åˆ é™¤: {curation_result.bullets_removed}")
    print(f"  - æ›´æ–°: {curation_result.bullets_updated}")
    print()


if __name__ == "__main__":
    main()
