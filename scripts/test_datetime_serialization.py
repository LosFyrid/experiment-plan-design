#!/usr/bin/env python3
"""æµ‹è¯• datetime åºåˆ—åŒ–ä¿®å¤

éªŒè¯ Feedback å¯¹è±¡èƒ½å¦æ­£ç¡®åºåˆ—åŒ–ä¸º JSON
"""

import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ  src åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from ace_framework.playbook.schemas import Feedback, FeedbackScore
from workflow.task_manager import GenerationTask, TaskStatus

def test_feedback_serialization():
    """æµ‹è¯• Feedback å¯¹è±¡çš„ JSON åºåˆ—åŒ–"""
    print("=" * 70)
    print("æµ‹è¯• Feedback JSON åºåˆ—åŒ–")
    print("=" * 70)

    # åˆ›å»ºæµ‹è¯• Feedback å¯¹è±¡
    feedback = Feedback(
        plan_id="test-plan-001",
        scores=[
            FeedbackScore(
                criterion="completeness",
                score=0.9,
                explanation="æ–¹æ¡ˆå®Œæ•´"
            ),
            FeedbackScore(
                criterion="safety",
                score=0.95,
                explanation="å®‰å…¨æç¤ºå……åˆ†"
            )
        ],
        overall_score=0.925,
        feedback_source="auto",
        comments="æµ‹è¯•åé¦ˆ",
        timestamp=datetime.now()
    )

    print(f"\nâœ… åˆ›å»º Feedback å¯¹è±¡:")
    print(f"   - Overall score: {feedback.overall_score}")
    print(f"   - Timestamp: {feedback.timestamp}")
    print(f"   - Scores: {len(feedback.scores)}")

    # æµ‹è¯• model_dump(mode='json')
    print(f"\nğŸ”§ æµ‹è¯• model_dump(mode='json')...")
    try:
        feedback_dict = feedback.model_dump(mode='json')
        print(f"âœ… model_dump() æˆåŠŸ")
        print(f"   - timestamp ç±»å‹: {type(feedback_dict['timestamp'])}")
        print(f"   - timestamp å€¼: {feedback_dict['timestamp']}")
    except Exception as e:
        print(f"âŒ model_dump() å¤±è´¥: {e}")
        return False

    # æµ‹è¯• JSON åºåˆ—åŒ–
    print(f"\nğŸ”§ æµ‹è¯• json.dump()...")
    import json
    import tempfile

    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:
            json.dump(feedback_dict, f, indent=2, ensure_ascii=False)
            temp_path = f.name

        print(f"âœ… json.dump() æˆåŠŸ")
        print(f"   - ä¸´æ—¶æ–‡ä»¶: {temp_path}")

        # è¯»å–å¹¶éªŒè¯
        with open(temp_path, 'r') as f:
            loaded = json.load(f)

        print(f"âœ… json.load() æˆåŠŸ")
        print(f"   - timestamp å€¼: {loaded['timestamp']}")

        # æ¸…ç†
        Path(temp_path).unlink()

    except Exception as e:
        print(f"âŒ json.dump() å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    print(f"\n{'=' * 70}")
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 70)
    return True


def test_task_save_feedback():
    """æµ‹è¯• GenerationTask.save_feedback() æ–¹æ³•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• GenerationTask.save_feedback()")
    print("=" * 70)

    import tempfile
    import shutil

    # åˆ›å»ºä¸´æ—¶ä»»åŠ¡ç›®å½•
    temp_dir = Path(tempfile.mkdtemp())
    print(f"\nğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")

    try:
        # åˆ›å»º GenerationTask å¯¹è±¡
        task = GenerationTask(
            task_id="test-task",
            session_id="test-session",
            status=TaskStatus.COMPLETED,
            task_dir=temp_dir
        )

        # åˆ›å»ºæµ‹è¯• Feedback
        feedback = Feedback(
            plan_id="test-plan-002",
            scores=[
                FeedbackScore(
                    criterion="clarity",
                    score=0.88,
                    explanation="æ¸…æ™°åº¦è‰¯å¥½"
                )
            ],
            overall_score=0.88,
            feedback_source="llm_judge",
            comments="LLMè¯„ä¼°ç»“æœ",
            timestamp=datetime.now()
        )

        print(f"\nğŸ”§ è°ƒç”¨ task.save_feedback()...")
        task.save_feedback(feedback)

        print(f"âœ… save_feedback() æˆåŠŸ")
        print(f"   - æ–‡ä»¶è·¯å¾„: {task.feedback_file}")
        print(f"   - æ–‡ä»¶å­˜åœ¨: {task.feedback_file.exists()}")

        # éªŒè¯æ–‡ä»¶å†…å®¹
        if task.feedback_file.exists():
            with open(task.feedback_file, 'r') as f:
                loaded = task.load_feedback()

            print(f"âœ… load_feedback() æˆåŠŸ")
            print(f"   - Overall score: {loaded['overall_score']}")
            print(f"   - Timestamp: {loaded['timestamp']}")

        print(f"\n{'=' * 70}")
        print("âœ… GenerationTask.save_feedback() æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir, ignore_errors=True)

    return True


if __name__ == "__main__":
    # æµ‹è¯•1: åŸºç¡€åºåˆ—åŒ–
    success1 = test_feedback_serialization()

    # æµ‹è¯•2: Task æ–¹æ³•
    success2 = test_task_save_feedback()

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"  åŸºç¡€åºåˆ—åŒ–: {'âœ… PASS' if success1 else 'âŒ FAIL'}")
    print(f"  GenerationTask.save_feedback(): {'âœ… PASS' if success2 else 'âŒ FAIL'}")

    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼datetime åºåˆ—åŒ–é—®é¢˜å·²ä¿®å¤ã€‚")
        sys.exit(0)
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¿®å¤ã€‚")
        sys.exit(1)
