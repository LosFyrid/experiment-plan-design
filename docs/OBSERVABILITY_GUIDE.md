# ACE Framework å¯è§‚æµ‹æ€§ä¸å¹²é¢„æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ç³»ç»Ÿçš„è§‚æµ‹ç‚¹ã€å¹²é¢„ç‚¹å’Œè°ƒè¯•ç­–ç•¥ã€‚

---

## ğŸ“Š å¯è§‚æµ‹æ€§å±‚çº§

### å½“å‰å®ç°çŠ¶æ€

```
è§‚æµ‹ç»´åº¦          | å®ç°çŠ¶æ€ | å¯è§‚æµ‹ç¨‹åº¦ | æ”¹è¿›å»ºè®®
-----------------|---------|-----------|----------
**æ•°æ®æµè¿½è¸ª**    | ğŸŸ¡ éƒ¨åˆ†  | ä¸­ç­‰      | æ·»åŠ ç»“æ„åŒ–æ—¥å¿—
**æ€§èƒ½ç›‘æ§**      | ğŸ”´ æ—     | ä½        | æ·»åŠ è®¡æ—¶å™¨å’ŒæŒ‡æ ‡
**é”™è¯¯è¿½è¸ª**      | ğŸŸ¡ éƒ¨åˆ†  | ä¸­ç­‰      | é›†ä¸­åŒ–é”™è¯¯å¤„ç†
**Playbookæ¼”åŒ–**  | ğŸŸ¢ å®Œæ•´  | é«˜        | å¯è§†åŒ–å·¥å…·
**LLMè°ƒç”¨ç›‘æ§**   | ğŸŸ¡ éƒ¨åˆ†  | ä¸­ç­‰      | æ·»åŠ token/costè¿½è¸ª
**ç»„ä»¶è¾“å‡º**      | ğŸŸ¢ å®Œæ•´  | é«˜        | å·²æœ‰ç»“æ„åŒ–è¾“å‡º
```

---

## ğŸ” è§‚æµ‹ç‚¹è¯¦è§£

### 1. Generatorè§‚æµ‹ç‚¹

#### è¾“å…¥è§‚æµ‹
```python
# ä½ç½®: generator.generate()
{
  "requirements": {...},           # ç”¨æˆ·éœ€æ±‚
  "templates": [...],              # RAGæ£€ç´¢çš„æ¨¡æ¿
  "section_filter": [...]          # ç­›é€‰çš„Playbook section
}
```

**è§‚æµ‹æ–¹æ³•**:
```python
# å½“å‰: æ— æ—¥å¿—
# å»ºè®®: æ·»åŠ æ—¥å¿—è®°å½•
logger.info(f"Generator input: requirements={requirements.keys()}, "
            f"templates_count={len(templates or [])}, "
            f"section_filter={section_filter}")
```

#### ä¸­é—´è¿‡ç¨‹è§‚æµ‹
```python
# æ­¥éª¤1: Bulletæ£€ç´¢
relevant_bullets = self._retrieve_bullets(requirements, section_filter)

# è§‚æµ‹ç‚¹1: æ£€ç´¢åˆ°çš„bullets
print(f"Retrieved {len(relevant_bullets)} bullets")
for bullet, similarity in relevant_bullets[:5]:  # æ‰“å°top-5
    print(f"  - {bullet.id}: {bullet.content[:50]}... (sim={similarity:.3f})")

# æ­¥éª¤2: æç¤ºè¯æ„å»º
user_prompt = build_generation_prompt(
    requirements=requirements,
    playbook_bullets=relevant_bullets,
    templates=templates,
    few_shot_examples=few_shot_examples
)

# è§‚æµ‹ç‚¹2: æç¤ºè¯é•¿åº¦
print(f"Prompt length: {len(user_prompt)} chars")
# å»ºè®®: ä¿å­˜åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
with open(f"logs/prompts/generator_{timestamp}.txt", "w") as f:
    f.write(user_prompt)

# æ­¥éª¤3: LLMè°ƒç”¨
response = self.llm.generate(prompt=user_prompt, system_prompt=SYSTEM_PROMPT)

# è§‚æµ‹ç‚¹3: LLMå“åº”
print(f"LLM response length: {len(response)} chars")
print(f"Tokens used: {self.llm.last_token_count}")  # å¦‚æœæœ‰tokenç»Ÿè®¡
```

#### è¾“å‡ºè§‚æµ‹
```python
# ä½ç½®: GenerationResult
result = GenerationResult(
    generated_plan=plan,            # ç”Ÿæˆçš„å®éªŒæ–¹æ¡ˆ
    relevant_bullets=bullet_ids,    # ä½¿ç”¨çš„bullet IDs
    trajectory=trajectory,          # æ¨ç†è½¨è¿¹
    metadata={
        "tokens_used": tokens,
        "generation_time": elapsed,
        "model": model_name
    }
)

# è§‚æµ‹æ–¹æ³•:
print(f"Generated plan: {result.generated_plan.title}")
print(f"Materials: {len(result.generated_plan.materials)}")
print(f"Procedure steps: {len(result.generated_plan.procedure)}")
print(f"Bullets used: {result.relevant_bullets}")
print(f"Trajectory steps: {len(result.trajectory)}")
```

**å½“å‰é—®é¢˜**:
- âŒ æ— ç»“æ„åŒ–æ—¥å¿—ï¼ˆåªæœ‰printè¯­å¥ï¼‰
- âŒ æ— æç¤ºè¯ä¿å­˜ï¼ˆéš¾ä»¥è°ƒè¯•LLMè¾“å‡ºï¼‰
- âŒ æ— æ€§èƒ½è®¡æ—¶ï¼ˆä¸çŸ¥é“ç“¶é¢ˆåœ¨å“ªï¼‰
- âŒ æ— tokenç»Ÿè®¡ï¼ˆæ— æ³•ä¼°ç®—æˆæœ¬ï¼‰

---

### 2. Reflectorè§‚æµ‹ç‚¹

#### è¾“å…¥è§‚æµ‹
```python
# ä½ç½®: reflector.reflect()
{
  "generated_plan": ExperimentPlan,  # Generatorçš„è¾“å‡º
  "feedback": Feedback,              # è¯„ä¼°ç»“æœ
  "trajectory": List[TrajectoryStep],# Generatoræ¨ç†è¿‡ç¨‹
  "playbook_bullets_used": List[str] # ä½¿ç”¨çš„bullet IDs
}
```

#### è¿­ä»£refinementè¿‡ç¨‹ï¼ˆæ ¸å¿ƒè§‚æµ‹ç‚¹ï¼‰

```python
# æ­¥éª¤1: åˆå§‹reflection
initial_output = self._perform_initial_reflection(...)

# è§‚æµ‹ç‚¹1: åˆå§‹insights
print(f"Initial reflection: {len(initial_output['insights'])} insights")
for insight in initial_output['insights']:
    print(f"  - [{insight['priority']}] {insight['type']}: {insight['description'][:50]}...")

# æ­¥éª¤2: è¿­ä»£refinement (max 5 rounds)
for round_num in range(2, self.config.max_refinement_rounds + 1):
    refined_output = self._refine_insights(
        previous_output=previous_output,
        round_num=round_num
    )

    # è§‚æµ‹ç‚¹2: æ¯è½®refinementçš„æ”¹è¿›
    print(f"\n--- Refinement Round {round_num} ---")
    print(f"Insights count: {len(refined_output['insights'])}")
    print(f"Quality improvement indicators:")
    print(f"  - High priority: {count_by_priority['high']}")
    print(f"  - Medium priority: {count_by_priority['medium']}")
    print(f"  - Low priority: {count_by_priority['low']}")

    # è§‚æµ‹ç‚¹3: Insightå†…å®¹å˜åŒ–
    # å»ºè®®: ä¿å­˜æ¯è½®è¾“å‡ºå¯¹æ¯”
    with open(f"logs/reflections/round_{round_num}.json", "w") as f:
        json.dump(refined_output, f, indent=2)
```

#### Bullet taggingè§‚æµ‹
```python
# æ­¥éª¤3: Bullet tagging
bullet_tags = self._extract_bullet_tags(final_output)

# è§‚æµ‹ç‚¹4: Bulletä½¿ç”¨æƒ…å†µ
helpful_count = sum(1 for tag in bullet_tags.values() if tag == BulletTag.HELPFUL)
harmful_count = sum(1 for tag in bullet_tags.values() if tag == BulletTag.HARMFUL)
neutral_count = sum(1 for tag in bullet_tags.values() if tag == BulletTag.NEUTRAL)

print(f"\nBullet Tagging Results:")
print(f"  Helpful: {helpful_count}")
print(f"  Harmful: {harmful_count}")
print(f"  Neutral: {neutral_count}")

for bullet_id, tag in bullet_tags.items():
    print(f"  {bullet_id}: {tag.value}")
```

**å½“å‰é—®é¢˜**:
- âš ï¸ æœ‰printè¾“å‡ºä½†æ— ç»“æ„åŒ–è®°å½•
- âŒ æ— refinementè¿‡ç¨‹çš„diffå¯¹æ¯”ï¼ˆæ— æ³•éªŒè¯æ˜¯å¦çœŸçš„æ”¹è¿›ï¼‰
- âŒ æ— è´¨é‡æŒ‡æ ‡é‡åŒ–ï¼ˆå¦‚insight specificity, actionabilityï¼‰

---

### 3. Curatorè§‚æµ‹ç‚¹ï¼ˆæœ€å…³é”®ï¼ï¼‰

#### Delta operationsç”Ÿæˆ

```python
# æ­¥éª¤1: ç”Ÿæˆdelta operations
delta_operations = self._generate_delta_operations(
    insights=reflection_result.insights,
    id_prefixes=id_prefixes
)

# è§‚æµ‹ç‚¹1: Delta operationsè¯¦æƒ…
print(f"\n=== Delta Operations ===")
add_ops = [op for op in delta_operations if op.operation == "ADD"]
update_ops = [op for op in delta_operations if op.operation == "UPDATE"]
remove_ops = [op for op in delta_operations if op.operation == "REMOVE"]

print(f"ADD: {len(add_ops)} operations")
for op in add_ops:
    print(f"  + [{op.new_bullet.section}] {op.new_bullet.content[:50]}...")

print(f"UPDATE: {len(update_ops)} operations")
for op in update_ops:
    print(f"  ~ {op.bullet_id}: {op.new_bullet.content[:50]}...")

print(f"REMOVE: {len(remove_ops)} operations")
for op in remove_ops:
    print(f"  - {op.bullet_id}: {op.reason}")
```

#### Deduplicationè§‚æµ‹ï¼ˆACE Â§3.2æ ¸å¿ƒï¼‰

```python
# æ­¥éª¤2: Semantic deduplication
dedup_report = self._perform_deduplication()

# è§‚æµ‹ç‚¹2: Deduplicationè¯¦æƒ…
print(f"\n=== Deduplication Report ===")
print(f"Total deduplicated: {dedup_report.total_deduplicated}")
print(f"Similarity threshold: {self.config.deduplication_threshold}")

for merge in dedup_report.merges:
    print(f"Merge: {merge['kept_bullet']} â† {merge['removed_bullet']}")
    print(f"  Similarity: {merge['similarity']:.3f}")
    print(f"  Kept helpfulness: {merge['kept_helpfulness']:.2f}")
    print(f"  Removed helpfulness: {merge['removed_helpfulness']:.2f}")
```

#### Playbookæ¼”åŒ–è§‚æµ‹ï¼ˆæœ€é‡è¦ï¼ï¼‰

```python
# æ­¥éª¤3: Playbook before/afterå¯¹æ¯”
print(f"\n=== Playbook Evolution ===")
print(f"Before: {playbook_before.size} bullets")
print(f"After: {playbook_after.size} bullets")
print(f"Net change: {playbook_after.size - playbook_before.size:+d}")

# è§‚æµ‹ç‚¹3: Sectionåˆ†å¸ƒå˜åŒ–
print("\nSection Distribution:")
for section in playbook_after.sections:
    before_count = len(playbook_before.get_bullets_by_section(section))
    after_count = len(playbook_after.get_bullets_by_section(section))
    print(f"  {section}: {before_count} â†’ {after_count} ({after_count - before_count:+d})")

# è§‚æµ‹ç‚¹4: Metadataç»Ÿè®¡
print("\nMetadata Statistics:")
avg_helpfulness_before = sum(b.metadata.helpfulness_score for b in playbook_before.bullets) / playbook_before.size
avg_helpfulness_after = sum(b.metadata.helpfulness_score for b in playbook_after.bullets) / playbook_after.size
print(f"  Avg helpfulness: {avg_helpfulness_before:.3f} â†’ {avg_helpfulness_after:.3f}")

# è§‚æµ‹ç‚¹5: æ–°å¢bulletsçš„æ¥æº
new_bullets = [b for b in playbook_after.bullets if b.metadata.source == "reflection"]
print(f"\nNew bullets from reflection: {len(new_bullets)}")
for bullet in new_bullets:
    print(f"  + {bullet.id}: {bullet.content[:60]}...")
```

**å½“å‰å®ç°**:
- ğŸŸ¢ å·²æœ‰printè¾“å‡ºï¼ˆcurator.py:165-498ï¼‰
- ğŸŸ¢ PlaybookUpdateResultåŒ…å«å®Œæ•´ç»Ÿè®¡
- âš ï¸ ä½†ç¼ºå°‘ç»“æ„åŒ–ä¿å­˜ï¼ˆæ— æ³•è¿½è¸ªå†å²ï¼‰

---

## ğŸ›ï¸ å¹²é¢„ç‚¹è¯¦è§£

### Tier 1: é…ç½®çº§å¹²é¢„ï¼ˆæœ€å¸¸ç”¨ï¼Œæ— éœ€æ”¹ä»£ç ï¼‰

#### 1.1 Generatorå¹²é¢„

```yaml
# configs/ace_config.yaml
generator:
  top_k_bullets: 50              # â† å¹²é¢„: å‡å°‘/å¢åŠ æ£€ç´¢çš„bulletsæ•°é‡
  enable_templates: true         # â† å¹²é¢„: å¼€å…³RAG template
  include_examples: false        # â† å¹²é¢„: å¼€å…³few-shot learning
  output_format: "json"          # â† å¹²é¢„: åˆ‡æ¢JSON/Markdownè¾“å‡º
  min_similarity: 0.3            # â† å¹²é¢„: Bulletæ£€ç´¢ç›¸ä¼¼åº¦é˜ˆå€¼
```

**æ•ˆæœ**:
- `top_k_bullets` â†‘ â†’ æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†promptæ›´é•¿ï¼Œæˆæœ¬æ›´é«˜
- `min_similarity` â†‘ â†’ æ›´ä¸¥æ ¼ç­›é€‰ï¼Œå¯èƒ½é—æ¼ç›¸å…³bullets
- `enable_templates: false` â†’ æµ‹è¯•Playbookå•ç‹¬çš„æ•ˆæœ

#### 1.2 Reflectorå¹²é¢„

```yaml
reflector:
  enable_iterative: true         # â† å¹²é¢„: å¼€å…³è¿­ä»£refinement
  max_refinement_rounds: 5       # â† å¹²é¢„: è°ƒæ•´refinementè½®æ•° (1-5)
  enable_bullet_tagging: true    # â† å¹²é¢„: å¼€å…³bullet tagging
  require_ground_truth: false    # â† å¹²é¢„: æ˜¯å¦éœ€è¦ground truthå¯¹æ¯”
```

**æ•ˆæœ**:
- `max_refinement_rounds: 1` â†’ è·³è¿‡è¿­ä»£ï¼Œæµ‹è¯•åˆå§‹reflectionè´¨é‡
- `max_refinement_rounds: 5` â†’ æœ€å¤§åŒ–insightè´¨é‡ï¼ˆè®ºæ–‡é»˜è®¤ï¼‰
- `enable_bullet_tagging: false` â†’ ç¦æ­¢Playbookå­¦ä¹ ï¼ˆåªæå–insightsï¼‰

#### 1.3 Curatorå¹²é¢„ï¼ˆæ ¸å¿ƒï¼ï¼‰

```yaml
curator:
  update_mode: "incremental"     # â† å¹²é¢„: incremental vs lazy
  enable_grow_and_refine: true   # â† å¹²é¢„: å¼€å…³deduplication
  deduplication_threshold: 0.85  # â† å¹²é¢„: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)
  max_playbook_size: 200         # â† å¹²é¢„: Playbookæœ€å¤§size
  pruning_threshold: 0.3         # â† å¹²é¢„: Helpfulnessé˜ˆå€¼ï¼ˆä½äºä¼šè¢«åˆ é™¤ï¼‰
```

**æ•ˆæœ**:
- `update_mode: "lazy"` â†’ åªåœ¨è¶…è¿‡max_sizeæ—¶deduplicateï¼ˆæ›´å¿«ï¼‰
- `deduplication_threshold` â†‘ (e.g., 0.95) â†’ æ›´ä¸¥æ ¼ï¼Œåªåˆ é™¤å‡ ä¹å®Œå…¨ç›¸åŒçš„
- `deduplication_threshold` â†“ (e.g., 0.75) â†’ æ›´æ¿€è¿›ï¼Œå¯èƒ½é”™åˆ æœ‰ç”¨çš„
- `max_playbook_size: 100` â†’ å¼ºåˆ¶æ›´é¢‘ç¹pruning
- `pruning_threshold: 0.5` â†’ æ›´æ¿€è¿›åˆ é™¤è¡¨ç°ä¸å¥½çš„bullets

#### 1.4 Modelå¹²é¢„

```yaml
model:
  provider: "qwen"              # â† å¹²é¢„: åˆ‡æ¢provider
  model_name: "qwen-max"        # â† å¹²é¢„: qwen-max vs qwen-plus vs qwen-turbo
  temperature: 0.7              # â† å¹²é¢„: 0=deterministic, 1=creative
  max_tokens: 4000              # â† å¹²é¢„: è¾“å‡ºé•¿åº¦é™åˆ¶
  top_p: 0.95                   # â† å¹²é¢„: nucleus sampling
```

**âš ï¸ é‡è¦**: æ‰€æœ‰ä¸‰ä¸ªè§’è‰²åº”ä½¿ç”¨**ç›¸åŒæ¨¡å‹**ï¼ˆè®ºæ–‡Â§4.2ï¼‰

---

### Tier 2: æ•°æ®çº§å¹²é¢„ï¼ˆæ‰‹åŠ¨ç¼–è¾‘ï¼‰

#### 2.1 Playbookç›´æ¥ç¼–è¾‘

```bash
# ç¼–è¾‘Playbookæ–‡ä»¶
vim data/playbooks/chemistry_playbook.json

# å¯ä»¥:
# - æ·»åŠ seed bullets (é«˜è´¨é‡èµ·ç‚¹)
# - åˆ é™¤æœ‰å®³bullets (äººå·¥å®¡æ ¸å)
# - ä¿®æ”¹bulletå†…å®¹ (çº æ­£é”™è¯¯)
# - è°ƒæ•´metadata (ä¿®æ­£helpfulness_score)
```

**ä½¿ç”¨åœºæ™¯**:
- **å†·å¯åŠ¨**: æ·»åŠ é¢†åŸŸä¸“å®¶çŸ¥è¯†ä½œä¸ºseed
- **è´¨é‡æ§åˆ¶**: äººå·¥å®¡æ ¸ååˆ é™¤æ˜æ˜¾é”™è¯¯çš„bullets
- **A/Bæµ‹è¯•**: åˆ›å»ºä¸åŒç‰ˆæœ¬çš„Playbookå¯¹æ¯”

#### 2.2 Promptç›´æ¥ä¿®æ”¹

```bash
# ä¿®æ”¹Generator prompts
vim src/ace_framework/generator/prompts.py

# ä¿®æ”¹Reflector prompts
vim src/ace_framework/reflector/prompts.py

# ä¿®æ”¹Curator prompts
vim src/ace_framework/curator/prompts.py
```

**âš ï¸ æ³¨æ„**: æŒ‰ç…§ä¹‹å‰çš„"ä½•æ—¶ä¿®æ”¹prompt"æŒ‡å—ï¼Œåªæ”¹domain-specificéƒ¨åˆ†

#### 2.3 Mock Feedbackè°ƒæ•´

```python
# åœ¨examples/ace_cycle_example.pyä¸­
feedback = Feedback(
    scores=[
        FeedbackScore(criterion="completeness", score=0.8, ...),
        FeedbackScore(criterion="safety", score=0.7, ...),  # â† å¹²é¢„: è°ƒæ•´è¯„åˆ†
        # ... æ·»åŠ æ›´å¤šcriteria
    ],
    overall_score=0.81,  # â† å¹²é¢„: è°ƒæ•´æ€»åˆ†
    feedback_source="auto"
)
```

**ä½¿ç”¨åœºæ™¯**:
- æµ‹è¯•Reflectorå¯¹ä¸åŒfeedbackçš„å“åº”
- æ¨¡æ‹Ÿæç«¯æƒ…å†µï¼ˆå…¨éƒ¨é«˜åˆ†/å…¨éƒ¨ä½åˆ†ï¼‰
- éªŒè¯ç‰¹å®šcriterionçš„å½±å“

---

### Tier 3: ä»£ç çº§å¹²é¢„ï¼ˆé«˜çº§è°ƒè¯•ï¼‰

#### 3.1 æ·»åŠ æ–­ç‚¹è°ƒè¯•

```python
# åœ¨generator.pyä¸­
def generate(self, requirements, templates, ...):
    relevant_bullets = self._retrieve_bullets(...)

    # å¹²é¢„ç‚¹1: æ£€æŸ¥æ£€ç´¢ç»“æœ
    import pdb; pdb.set_trace()  # æ–­ç‚¹

    user_prompt = build_generation_prompt(...)

    # å¹²é¢„ç‚¹2: æ£€æŸ¥æç¤ºè¯
    import pdb; pdb.set_trace()

    response = self.llm.generate(...)

    # å¹²é¢„ç‚¹3: æ£€æŸ¥LLMå“åº”
    import pdb; pdb.set_trace()
```

#### 3.2 æ·»åŠ hookå‡½æ•°

```python
# åœ¨generator.pyçš„__init__ä¸­
self.before_generate_hook = None  # ç”¨æˆ·å¯æ³¨å…¥
self.after_generate_hook = None

def generate(self, ...):
    if self.before_generate_hook:
        self.before_generate_hook(requirements, templates)

    result = ...  # æ­£å¸¸ç”Ÿæˆ

    if self.after_generate_hook:
        self.after_generate_hook(result)

    return result

# ä½¿ç”¨:
def my_hook(result):
    print(f"Custom logging: {result.generated_plan.title}")
    # ä¿å­˜åˆ°è‡ªå®šä¹‰æ—¥å¿—ç³»ç»Ÿ

generator.after_generate_hook = my_hook
```

#### 3.3 Replayæœºåˆ¶ï¼ˆæœªå®ç°ï¼Œå»ºè®®ï¼‰

```python
# å»ºè®®: ä¿å­˜æ¯æ¬¡ç”Ÿæˆçš„å®Œæ•´è¾“å…¥/è¾“å‡º
class ReplayableGenerator(PlanGenerator):
    def generate(self, ...):
        # ä¿å­˜è¾“å…¥
        with open(f"logs/replays/{timestamp}_input.json", "w") as f:
            json.dump({
                "requirements": requirements,
                "templates": templates,
                "config": self.config.dict()
            }, f)

        result = super().generate(...)

        # ä¿å­˜è¾“å‡º
        with open(f"logs/replays/{timestamp}_output.json", "w") as f:
            json.dump(result.dict(), f)

        return result

# å¥½å¤„: å¯ä»¥é‡æ”¾ä»»æ„å†å²ç”Ÿæˆï¼Œç”¨äº:
# - Bugå¤ç°
# - æ¨¡å‹å¯¹æ¯”ï¼ˆé‡æ”¾åŒæ ·è¾“å…¥åˆ°ä¸åŒæ¨¡å‹ï¼‰
# - Promptä¼˜åŒ–ï¼ˆé‡æ”¾åŒæ ·è¾“å…¥åˆ°ä¸åŒpromptç‰ˆæœ¬ï¼‰
```

---

## ğŸ› ï¸ æ¨èçš„å¯è§‚æµ‹æ€§å¢å¼ºæ–¹æ¡ˆ

### ä¼˜å…ˆçº§1: ç»“æ„åŒ–æ—¥å¿—

```python
# åˆ›å»º: src/utils/logger.py
import logging
import json
from pathlib import Path
from datetime import datetime

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨ï¼Œæ”¯æŒJSONè¾“å‡ºå’ŒæŸ¥è¯¢"""

    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        # åˆ›å»ºåˆ†ç±»æ—¥å¿—
        self.generator_log = self.log_dir / "generator.jsonl"
        self.reflector_log = self.log_dir / "reflector.jsonl"
        self.curator_log = self.log_dir / "curator.jsonl"

    def log_generation(self, event_type: str, data: dict):
        """è®°å½•Generatoräº‹ä»¶"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "component": "generator",
            "event_type": event_type,  # "bullet_retrieval", "llm_call", "output"
            "data": data
        }
        with open(self.generator_log, "a") as f:
            f.write(json.dumps(entry) + "\n")

    def log_reflection(self, event_type: str, data: dict):
        """è®°å½•Reflectoräº‹ä»¶"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "component": "reflector",
            "event_type": event_type,  # "initial", "refinement_round", "bullet_tagging"
            "data": data
        }
        with open(self.reflector_log, "a") as f:
            f.write(json.dumps(entry) + "\n")

    def log_curation(self, event_type: str, data: dict):
        """è®°å½•Curatoräº‹ä»¶"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "component": "curator",
            "event_type": event_type,  # "delta_ops", "deduplication", "pruning"
            "data": data
        }
        with open(self.curator_log, "a") as f:
            f.write(json.dumps(entry) + "\n")

# ä½¿ç”¨:
logger = StructuredLogger()
logger.log_generation("bullet_retrieval", {
    "query": requirements["objective"],
    "bullets_retrieved": len(relevant_bullets),
    "top_5_similarities": [sim for _, sim in relevant_bullets[:5]]
})
```

### ä¼˜å…ˆçº§2: Playbookç‰ˆæœ¬è¿½è¸ª

```python
# åˆ›å»º: src/utils/playbook_versioning.py
import json
import shutil
from pathlib import Path
from datetime import datetime

class PlaybookVersionTracker:
    """è¿½è¸ªPlaybookæ¼”åŒ–å†å²"""

    def __init__(self, playbook_path: str, versions_dir: str = "data/playbook_versions"):
        self.playbook_path = Path(playbook_path)
        self.versions_dir = Path(versions_dir)
        self.versions_dir.mkdir(exist_ok=True)

    def save_version(self, reason: str, metadata: dict = None):
        """ä¿å­˜å½“å‰Playbookç‰ˆæœ¬å¿«ç…§"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version_file = self.versions_dir / f"playbook_{timestamp}.json"

        # å¤åˆ¶Playbook
        shutil.copy(self.playbook_path, version_file)

        # ä¿å­˜å…ƒæ•°æ®
        meta_file = self.versions_dir / f"meta_{timestamp}.json"
        with open(meta_file, "w") as f:
            json.dump({
                "timestamp": timestamp,
                "reason": reason,
                "metadata": metadata or {}
            }, f, indent=2)

        return version_file

    def diff_versions(self, version1: str, version2: str):
        """å¯¹æ¯”ä¸¤ä¸ªPlaybookç‰ˆæœ¬"""
        with open(version1) as f:
            pb1 = json.load(f)
        with open(version2) as f:
            pb2 = json.load(f)

        bullets1 = {b["id"]: b for b in pb1["bullets"]}
        bullets2 = {b["id"]: b for b in pb2["bullets"]}

        added = set(bullets2.keys()) - set(bullets1.keys())
        removed = set(bullets1.keys()) - set(bullets2.keys())
        modified = {
            bid for bid in bullets1.keys() & bullets2.keys()
            if bullets1[bid]["content"] != bullets2[bid]["content"]
        }

        return {
            "added": list(added),
            "removed": list(removed),
            "modified": list(modified),
            "size_change": len(bullets2) - len(bullets1)
        }

# ä½¿ç”¨:
tracker = PlaybookVersionTracker(playbook_path)
tracker.save_version(
    reason="After ACE cycle",
    metadata={
        "generation_id": gen_id,
        "feedback_score": 0.81,
        "insights_count": 3
    }
)
```

### ä¼˜å…ˆçº§3: æ€§èƒ½ç›‘æ§

```python
# åˆ›å»º: src/utils/performance_monitor.py
import time
from contextlib import contextmanager
from typing import Dict, List
import json

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics: Dict[str, List[float]] = {}

    @contextmanager
    def measure(self, operation: str):
        """æµ‹é‡æ“ä½œè€—æ—¶"""
        start = time.time()
        try:
            yield
        finally:
            elapsed = time.time() - start
            if operation not in self.metrics:
                self.metrics[operation] = []
            self.metrics[operation].append(elapsed)

    def report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {}
        for operation, times in self.metrics.items():
            report[operation] = {
                "count": len(times),
                "total": sum(times),
                "avg": sum(times) / len(times),
                "min": min(times),
                "max": max(times)
            }
        return report

    def save(self, path: str):
        """ä¿å­˜æ€§èƒ½æ•°æ®"""
        with open(path, "w") as f:
            json.dump(self.report(), f, indent=2)

# ä½¿ç”¨:
perf = PerformanceMonitor()

with perf.measure("bullet_retrieval"):
    relevant_bullets = self._retrieve_bullets(...)

with perf.measure("llm_generation"):
    response = self.llm.generate(...)

with perf.measure("output_parsing"):
    plan = ExperimentPlan(**parsed)

# æœ€å:
perf.save("logs/performance.json")
print(perf.report())
```

---

## ğŸ“ˆ å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ

### 1. å…³é”®äº‹ä»¶å¿…é¡»è®°å½•

```python
# Generator
âœ“ Bulletæ£€ç´¢ (æ•°é‡ã€ç›¸ä¼¼åº¦åˆ†å¸ƒ)
âœ“ LLMè°ƒç”¨ (è¾“å…¥/è¾“å‡ºtokenæ•°ã€è€—æ—¶)
âœ“ è¾“å‡ºè§£æ (æˆåŠŸ/å¤±è´¥)

# Reflector
âœ“ æ¯è½®refinement (insightsæ•°é‡ã€ä¼˜å…ˆçº§åˆ†å¸ƒ)
âœ“ Bullet tagging (helpful/harmful/neutralç»Ÿè®¡)

# Curator
âœ“ Delta operations (ADD/UPDATE/REMOVEå„å‡ ä¸ª)
âœ“ Deduplication (åˆ é™¤äº†å‡ ä¸ªã€ç›¸ä¼¼åº¦åˆ†å¸ƒ)
âœ“ Pruning (åˆ é™¤äº†å‡ ä¸ªã€åŸå› )
âœ“ Playbook sizeå˜åŒ– (before/after)
```

### 2. é”™è¯¯å¿…é¡»å¯è¿½æº¯

```python
# å½“å‰é—®é¢˜: åªæœ‰printè­¦å‘Š
print(f"Warning: Failed to parse JSON: {e}")

# æ”¹è¿›: ä¿å­˜å®Œæ•´é”™è¯¯ä¸Šä¸‹æ–‡
logger.error("JSON parsing failed", extra={
    "raw_response": response,
    "error": str(e),
    "traceback": traceback.format_exc(),
    "prompt": user_prompt,  # â† å…³é”®! å¯ä»¥é‡æ”¾
    "config": self.config.dict()
})
```

### 3. ä¸­é—´ç»“æœå¯å¯¼å‡º

```python
# æ¯ä¸ªç»„ä»¶åº”æ”¯æŒå¯¼å‡ºä¸­é—´çŠ¶æ€
result = generator.generate(...)

# å¯¼å‡ºé€‰é¡¹1: å®Œæ•´JSON
result.export("logs/generations/gen_20250123_143022.json")

# å¯¼å‡ºé€‰é¡¹2: äººç±»å¯è¯»
result.export_human_readable("logs/generations/gen_20250123_143022.md")

# å†…å®¹åº”åŒ…æ‹¬:
# - è¾“å…¥ (requirements, templates)
# - ä¸­é—´æ­¥éª¤ (bulletsæ£€ç´¢, promptæ„å»º)
# - LLMäº¤äº’ (request, response)
# - è¾“å‡º (parsed plan)
# - Metadata (tokens, time, config)
```

### 4. æ”¯æŒæ—¶é—´èŒƒå›´æŸ¥è¯¢

```bash
# æŸ¥è¯¢è¿‡å»24å°æ—¶çš„Playbookå˜åŒ–
python scripts/query_logs.py --component curator \
    --event-type deduplication \
    --since "2025-01-23 00:00:00" \
    --format table

# è¾“å‡º:
# Timestamp            | Deduplicated | Avg Similarity | Playbook Size
# ---------------------|--------------|----------------|---------------
# 2025-01-23 10:30:15  | 3            | 0.91           | 47 â†’ 44
# 2025-01-23 14:22:08  | 1            | 0.87           | 50 â†’ 49
```

---

## ğŸ¯ æ¨èçš„è°ƒè¯•å·¥ä½œæµ

### åœºæ™¯1: Generatorè¾“å‡ºè´¨é‡ä¸å¥½

```
1. æ£€æŸ¥bulletæ£€ç´¢
   â†’ æŸ¥çœ‹logs/generator.jsonlä¸­çš„bullet_retrievaläº‹ä»¶
   â†’ éªŒè¯ç›¸ä¼¼åº¦åˆ†å¸ƒæ˜¯å¦åˆç†ï¼ˆæ˜¯å¦æ£€ç´¢åˆ°ç›¸å…³bullets?ï¼‰

2. æ£€æŸ¥prompt
   â†’ å¯¼å‡ºpromptåˆ°logs/prompts/
   â†’ äººå·¥é˜…è¯»ï¼ŒéªŒè¯instructionsæ˜¯å¦æ¸…æ™°
   â†’ å°è¯•è°ƒæ•´top_k_bulletsæˆ–min_similarity

3. æ£€æŸ¥LLMè¾“å‡º
   â†’ æŸ¥çœ‹logs/generator.jsonlä¸­çš„llm_calläº‹ä»¶
   â†’ éªŒè¯æ˜¯å¦æ˜¯parsingé”™è¯¯è¿˜æ˜¯LLMç”Ÿæˆè´¨é‡é—®é¢˜
   â†’ å°è¯•è°ƒæ•´temperatureæˆ–åˆ‡æ¢æ¨¡å‹
```

### åœºæ™¯2: Playbookå¢é•¿è¿‡å¿«

```
1. æ£€æŸ¥delta operations
   â†’ æŸ¥çœ‹logs/curator.jsonlä¸­çš„delta_opsäº‹ä»¶
   â†’ ç»Ÿè®¡ADDæ“ä½œå æ¯”ï¼ˆæ˜¯å¦æ¯æ¬¡éƒ½æ·»åŠ å¤§é‡bullets?ï¼‰

2. æ£€æŸ¥deduplication
   â†’ æŸ¥çœ‹logs/curator.jsonlä¸­çš„deduplicationäº‹ä»¶
   â†’ éªŒè¯thresholdæ˜¯å¦è¿‡é«˜ï¼ˆå¯¼è‡´æ— æ³•åˆ é™¤ç›¸ä¼¼bulletsï¼‰
   â†’ å°è¯•é™ä½deduplication_threshold (e.g., 0.85 â†’ 0.80)

3. å¯ç”¨æ›´æ¿€è¿›pruning
   â†’ è°ƒæ•´configs/ace_config.yaml:
     curator:
       max_playbook_size: 100  # é™ä½ä¸Šé™
       pruning_threshold: 0.4  # æé«˜pruningé˜ˆå€¼
```

### åœºæ™¯3: Refinementæ²¡æœ‰æ”¹è¿›insights

```
1. å¯¼å‡ºæ¯è½®refinementè¾“å‡º
   â†’ ä½¿ç”¨logger.log_reflectionä¿å­˜æ¯è½®è¾“å‡º
   â†’ äººå·¥å¯¹æ¯”round 1 vs round 5çš„insights

2. éªŒè¯promptæ˜¯å¦æœ‰æ•ˆ
   â†’ æ£€æŸ¥prompts.pyä¸­çš„REFINEMENT_SYSTEM_PROMPT
   â†’ ç¡®è®¤æ˜¯å¦æ¸…æ¥šæŒ‡ç¤º"improve specificity and actionability"

3. å°è¯•ä¸åŒrefinementè½®æ•°
   â†’ max_refinement_rounds: 1 (baseline)
   â†’ max_refinement_rounds: 3
   â†’ max_refinement_rounds: 5
   â†’ å¯¹æ¯”è´¨é‡å·®å¼‚
```

---

## ğŸ“Š å¯è§†åŒ–å·¥å…·ï¼ˆå»ºè®®å®ç°ï¼‰

### 1. Playbookæ¼”åŒ–å¯è§†åŒ–

```python
# scripts/visualize_playbook_evolution.py
import matplotlib.pyplot as plt
import json
from pathlib import Path

def plot_evolution(versions_dir: str):
    versions = sorted(Path(versions_dir).glob("playbook_*.json"))

    sizes = []
    avg_helpfulness = []
    timestamps = []

    for version_file in versions:
        with open(version_file) as f:
            playbook = json.load(f)

        sizes.append(len(playbook["bullets"]))

        helpfulness_scores = [
            b["metadata"]["helpful_count"] / max(b["metadata"]["total_uses"], 1)
            for b in playbook["bullets"]
        ]
        avg_helpfulness.append(sum(helpfulness_scores) / len(helpfulness_scores))

        # ä»æ–‡ä»¶åæå–æ—¶é—´
        timestamp = version_file.stem.split("_", 1)[1]
        timestamps.append(timestamp)

    # ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    ax1.plot(timestamps, sizes, marker='o')
    ax1.set_title("Playbook Size Over Time")
    ax1.set_ylabel("Number of Bullets")

    ax2.plot(timestamps, avg_helpfulness, marker='o', color='green')
    ax2.set_title("Average Helpfulness Score Over Time")
    ax2.set_ylabel("Helpfulness Score")

    plt.tight_layout()
    plt.savefig("logs/playbook_evolution.png")
    print("Saved to logs/playbook_evolution.png")

# ä½¿ç”¨:
plot_evolution("data/playbook_versions")
```

### 2. Bulletçƒ­åŠ›å›¾

```python
# å¯è§†åŒ–å“ªäº›bulletsè¢«é¢‘ç¹ä½¿ç”¨
def plot_bullet_heatmap(playbook_path: str):
    with open(playbook_path) as f:
        playbook = json.load(f)

    # æŒ‰sectionåˆ†ç»„ï¼Œç»Ÿè®¡total_uses
    section_usage = {}
    for bullet in playbook["bullets"]:
        section = bullet["section"]
        uses = bullet["metadata"]["total_uses"]

        if section not in section_usage:
            section_usage[section] = []
        section_usage[section].append(uses)

    # ç»˜åˆ¶heatmap
    # (å®ç°ç•¥)
```

---

## âœ… æ€»ç»“ï¼šå¿«é€Ÿå¯è§‚æµ‹æ€§æ£€æŸ¥æ¸…å•

åœ¨è¿è¡ŒACEå¾ªç¯åï¼Œæ£€æŸ¥ä»¥ä¸‹å†…å®¹ï¼š

- [ ] **Generatoræ—¥å¿—**: æ£€æŸ¥æ£€ç´¢åˆ°çš„bulletsæ•°é‡å’Œç›¸ä¼¼åº¦
- [ ] **Reflectoræ—¥å¿—**: æŸ¥çœ‹æ¯è½®refinementçš„insightsæ•°é‡
- [ ] **Curatoræ—¥å¿—**: ç¡®è®¤delta operationsç±»å‹å’Œæ•°é‡
- [ ] **DeduplicationæŠ¥å‘Š**: éªŒè¯æ˜¯å¦æœ‰é‡å¤bulletsè¢«åˆå¹¶
- [ ] **Playbookæ–‡ä»¶**: å¯¹æ¯”before/afterï¼Œç¡®è®¤sizeå˜åŒ–åˆç†
- [ ] **Playbookç‰ˆæœ¬**: ä¿å­˜å¿«ç…§ï¼Œä¾¿äºå›æ»š
- [ ] **æ€§èƒ½æ•°æ®**: æ£€æŸ¥å“ªä¸ªæ­¥éª¤æœ€è€—æ—¶
- [ ] **é”™è¯¯æ—¥å¿—**: æŸ¥çœ‹æ˜¯å¦æœ‰parsingæˆ–LLMè°ƒç”¨å¤±è´¥

å½“å‰æœ€éœ€è¦çš„æ”¹è¿›:
1. ğŸ”´ æ·»åŠ ç»“æ„åŒ–æ—¥å¿—ï¼ˆStructuredLoggerï¼‰
2. ğŸ”´ å®ç°Playbookç‰ˆæœ¬è¿½è¸ªï¼ˆPlaybookVersionTrackerï¼‰
3. ğŸŸ¡ æ·»åŠ æ€§èƒ½ç›‘æ§ï¼ˆPerformanceMonitorï¼‰
4. ğŸŸ¡ å®ç°å¯è§†åŒ–å·¥å…·

è¿™äº›æ”¹è¿›å¯ä»¥è®©ä½ æ›´å¥½åœ°ç†è§£å’Œè°ƒè¯•ACEæ¡†æ¶çš„è¿è¡ŒçŠ¶æ€ï¼
